jobs in vsc

job in sbatch zetten: sbatch --export=NrOfVars=28,Iters=1,Vtree="balanced",Overhead="true" exampleSbatchScript.slurm

requesting compute resources

	nrOfTasks --ntasks=<number of tasks>
	cores per task --cpus-per-task=<value>
	
	Make sure to request a valid combination of tasks and/or cores per task.
	
requesting memory 
	mem per cpy -mem-per-cpu=<amount><unit> (e.g., --mem-per-cpu=1g)
	The amount is an integer, <unit> can be either k for kilobytes, m for megabyte or g for gigabyte
	
	If --mem-per-cpu is not set, a default value will be used, which is usually equal to the total memory available for job allocations of that node divided by the number of cores.

	The amount of available memory per core is available via the variable SLURM_MEM_PER_CPU as an integer with megabytes as unit in the environment of the running job.
	
requesting walltime
	compute time --time=<time>. <time> is specified in mm (minutes), mm:ss (minutes and seconds), hh:mm:ss (hours, minutes and seconds), 
	d-hh (days and hours), d-hh:mm (days, hours and minutes) or d-hh:mm:ss (days, hours, minutes and seconds) format.
	
mail on event
	--mail-type=<type>. <type> is a comma-separated list of type values. Type values include BEGIN, END and FAIL
	--mail-user=<mail address> 

check which modules are available (bv voor iPython):
 	ml av ipython
 	->mpi4py module: mpi4py/3.1.4-gompi-2022b
 	
loading modules:
	module load <name>

resetting all modules:
	module purge
	
kul cluster + partition
	--clusters=wice
	--partition=batch (default, 72 tasks per node)
voorbeeld voor mpi
	#! /bin/bash
	#
	#SBATCH --clusters=wice
	#SBATCH --job-name=sddCompilationTest
	#SBATCH --nodes=1
	#SBATCH --ntasks-per-node=72	//--cpus-per-task=1 --mem-per-cpu=512m //-> two nodes on a cluster with 28 cores per node.
	#SBATCH --time=5:00

	# Build the environment (UAntwerp example)
	module --force purge
	module load calcua/2020a
	module load vsc-tutorial

	mpirun python mpi_script.py
	
to run a python script
	mpirun python mpi_script.py
--------------------------------------------------------------------------------------------------------------------------------------

submit job
	sbatch --account=intro_vsc3xxxx run_mpi_job.slurm
	
copying files from pc to servers:
 	scp local_file.txt <vsc-account>@<vsc-loginnode>:
copying files from server to pc:
	scp <vsc-account>@<vsc-loginnode>:remote_file.txt
copying dirs: 
	scp -r inputs/ vsc50005@login.hpc.kuleuven.be:/data/leuven/500/vsc50005/
	
login-nodes:
	login1-tier2.hpc.kuleuven.be 	
	
credit overzicht commands:
	sam-balance, sam-list-usagerecords, sam-list-allocations and sam-statement
 	
modules for python:
	Note that some packages, e.g., mpi4py, pyh5, pytables,â€¦, are available through the module system, and have to be loaded separately. 
	These packages will not be listed by pip unless you loaded the corresponding module. 
	In recent toolchains, many of the packages you need for scientific computing have been bundled into the SciPy-bundle module.
 
 
 	
 	
 	
 	
 	
 	

